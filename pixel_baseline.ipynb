{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h6v4c9_UwOmr",
    "outputId": "d5b5140a-cd61-45f6-859d-79986d112476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import math\n",
    "import gym \n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import warnings\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "from collections import Counter, deque\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm.notebook import tqdm\n",
    "from gym.wrappers import Monitor\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "import io\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import kornia.augmentation as aug\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XREqct6awOms"
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "# display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "T1ACavBgwOmt"
   },
   "outputs": [],
   "source": [
    "import os, sys, copy, argparse, shutil\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Deep Q Network Argument Parser')\n",
    "    parser.add_argument('--seed', dest='seed', type=int, default=1)\n",
    "    parser.add_argument('--env', dest='env', type=str, default='CartPole-v0')\n",
    "    parser.add_argument('--save_interval', type=int, default=50, help='save model every n episodes')\n",
    "    parser.add_argument('--log_interval', type=int, default=10, help='logging every n episodes')\n",
    "    parser.add_argument('--render', help='render', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', help='batch_size', type=int, default=32)\n",
    "    parser.add_argument('--train_freq', help='train_frequency', type=int, default=1)\n",
    "    parser.add_argument('--max_episode', help='maximum episode', type=int, default=None)\n",
    "    parser.add_argument('--max_timesteps', help='maximum timestep', type=int, default=100000000)\n",
    "    parser.add_argument('--lr', dest='lr', type=float, default=0.00025)\n",
    "    parser.add_argument('--lr_decay', action='store_true', help='decay learning rate')\n",
    "    parser.add_argument('--gamma', help='discount_factor', type=float, default=0.99)\n",
    "    parser.add_argument('--warmup_mem', type=int, help='warmup memory size', default=1000)\n",
    "    parser.add_argument('--frame_skip', type=int, help='number of frames to skip for each action', default=3)\n",
    "    parser.add_argument('--frame_stack', type=int, help='number of frames to stack', default=4)\n",
    "    parser.add_argument('--memory', help='memory size', type=int, default=1000000)\n",
    "    parser.add_argument('--initial_epsilon', '-ie', help='initial_epsilon', type=float, default=0.5)\n",
    "    parser.add_argument('--final_epsilon', '-fe', help='final_epsilon', type=float, default=0.05)\n",
    "    parser.add_argument('--max_epsilon_decay_steps', '-eds', help='maximum steps to decay epsilon', type=int, default=100000)\n",
    "    parser.add_argument('--max_grad_norm', type=float, default=None, help='maximum gradient norm')\n",
    "    parser.add_argument('--soft_update', '-su', action='store_true', help='soft update target network')\n",
    "    parser.add_argument('--double_q', '-dq', action='store_true', help='enabling double DQN')\n",
    "    parser.add_argument('--dueling_net', '-dn', action='store_true', help='enabling dueling network')\n",
    "    parser.add_argument('--test', action='store_true', help='test the trained model')\n",
    "    parser.add_argument('--tau', type=float, default=0.01, help='tau for soft target network update')\n",
    "    parser.add_argument('--hard_update_freq', '-huf', type=int, default=2000, help='hard target network update frequency')\n",
    "    parser.add_argument('--save_dir', type=str, default='./data')\n",
    "    parser.add_argument('--resume_step', '-rs', type=int, default=None)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X3Rkxe4VwOmu"
   },
   "outputs": [],
   "source": [
    "#@title Set up constants for env and training\n",
    "test = False \n",
    "save_dir = './data'\n",
    "render = False\n",
    "max_episode = None\n",
    "max_timesteps = 100000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "539HMtWFwOmv"
   },
   "outputs": [],
   "source": [
    "#@title Augmentations\n",
    "\n",
    "color_jitter = aug.ColorJitter(\n",
    "        brightness=np.random.random(),\n",
    "        contrast=np.random.random(),\n",
    "        saturation=np.random.random(),\n",
    "        hue=np.random.random(),\n",
    "        p=0.5\n",
    "        )\n",
    "random_elastic_transform = aug.RandomElasticTransform()\n",
    "random_fisheye = aug.RandomFisheye(\n",
    "        center_x=torch.tensor([-.3, .3]),\n",
    "        center_y=torch.tensor([-.3, .3]),\n",
    "        gamma=torch.tensor([.9, 1.]),\n",
    "        )\n",
    "random_color_equalize = aug.RandomEqualize()\n",
    "random_gaussian_blur = aug.RandomGaussianBlur(\n",
    "        kernel_size=(9, 9),\n",
    "        sigma = (5., 5.)\n",
    "        )\n",
    "random_gaussian_noise = aug.RandomGaussianNoise()\n",
    "random_horizontal_flip = aug.RandomHorizontalFlip()\n",
    "random_color_invert = aug.RandomInvert()\n",
    "random_perspective_shift = aug.RandomPerspective()\n",
    "random_shift = nn.Sequential(aug.RandomCrop((190, 140)), nn.ReplicationPad2d(20), aug.RandomCrop((210, 160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "K78NVCz-oqHc"
   },
   "outputs": [],
   "source": [
    "def tie_weights(src, trg):\n",
    "    assert type(src) == type(trg)\n",
    "    trg.weight = src.weight\n",
    "    trg.bias = src.bias\n",
    "\n",
    "\n",
    "# for 84 x 84 inputs\n",
    "OUT_DIM = {2: 39, 4: 35, 6: 31}\n",
    "# for 64 x 64 inputs\n",
    "OUT_DIM_64 = {2: 29, 4: 25, 6: 21}\n",
    "\n",
    "''' TODO change the layer parameters ''' \n",
    "class PixelEncoder(nn.Module):\n",
    "    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n",
    "    # [210, 160] --> crop [190, 140]\n",
    "    def __init__(self, obs_shape, feature_dim=50, num_layers=3, num_filters=64, output_logits=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(obs_shape) == 3\n",
    "        self.obs_shape = obs_shape\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 160, 210, 3 \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=5)\n",
    "        # Input to conv2: 32, 42, 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # Input to conv3: 15, 20, 64\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=4, stride=1)\n",
    "        # Output from conv3: 12, 17, 64\n",
    "\n",
    "        # out_dim = OUT_DIM_64[num_layers] if obs_shape[-1] == 64 else OUT_DIM[num_layers]\n",
    "        out_dims = (12, 17)\n",
    "        self.fc = nn.Linear(num_filters * out_dims[0] * out_dims[1], self.feature_dim)\n",
    "        self.ln = nn.LayerNorm(self.feature_dim)\n",
    "\n",
    "        self.outputs = dict()\n",
    "        self.output_logits = output_logits\n",
    "\n",
    "    def reparameterize(self, mu, logstd):\n",
    "        std = torch.exp(logstd)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward_conv(self, obs):\n",
    "        obs = obs # / 255.(already handled in agent.remember())\n",
    "        self.outputs['obs'] = obs\n",
    "        conv1 = torch.relu(self.conv1(obs))\n",
    "        self.outputs['conv1'] = conv1\n",
    "        conv2 = torch.relu(self.conv2(conv1))\n",
    "        self.outputs['conv2'] = conv2\n",
    "        conv3 = torch.relu(self.conv3(conv2))\n",
    "        self.outputs['conv3'] = conv3\n",
    "\n",
    "        h = conv3.view(conv3.size(0), -1)\n",
    "        return h\n",
    "\n",
    "    def forward(self, obs, detach=False):\n",
    "        h = self.forward_conv(obs)\n",
    "\n",
    "        if detach:\n",
    "            h = h.detach()\n",
    "\n",
    "        h_fc = self.fc(h)\n",
    "        self.outputs['fc'] = h_fc\n",
    "\n",
    "        h_norm = self.ln(h_fc)\n",
    "        self.outputs['ln'] = h_norm\n",
    "\n",
    "        if self.output_logits:\n",
    "            out = h_norm\n",
    "        else:\n",
    "            out = torch.tanh(h_norm)\n",
    "            self.outputs['tanh'] = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def copy_conv_weights_from(self, source):\n",
    "        \"\"\"Tie convolutional layers\"\"\"\n",
    "        # only tie conv layers\n",
    "        for i in range(self.num_layers):\n",
    "            tie_weights(src=source.convs[i], trg=self.convs[i])\n",
    "\n",
    "    def log(self, L, step, log_freq):\n",
    "        if step % log_freq != 0:\n",
    "            return\n",
    "\n",
    "        for k, v in self.outputs.items():\n",
    "            L.log_histogram('train_encoder/%s_hist' % k, v, step)\n",
    "            if len(v.shape) > 2:\n",
    "                L.log_image('train_encoder/%s_img' % k, v[0], step)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            L.log_param('train_encoder/conv%s' % (i + 1), self.convs[i], step)\n",
    "        L.log_param('train_encoder/fc', self.fc, step)\n",
    "        L.log_param('train_encoder/ln', self.ln, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0kIzyV1QwOmw"
   },
   "outputs": [],
   "source": [
    "def wrap_env(env, train=True):\n",
    "    suffix = 'train' if train else 'test'\n",
    "    monitor_dir = os.path.join(save_dir, 'monitor_%s' % suffix)\n",
    "    os.makedirs(monitor_dir, exist_ok=True)\n",
    "    if not train:\n",
    "        video_save_interval = 10\n",
    "        env = Monitor(env, directory=monitor_dir,\n",
    "                      video_callable=lambda episode_id: episode_id % video_save_interval == 0,\n",
    "                      force=True)\n",
    "    else:\n",
    "        if render:\n",
    "            if max_episode is not None:\n",
    "                video_save_interval = int(max_episode / 3)\n",
    "            else:\n",
    "                video_save_interval = int(max_timesteps / float(env._max_episode_steps) / 3)\n",
    "            env = Monitor(env, directory=monitor_dir,\n",
    "                          video_callable=lambda episode_id: episode_id % video_save_interval == 0,\n",
    "                          force=True)\n",
    "        else:\n",
    "            env = Monitor(env, directory=monitor_dir, video_callable=False, force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "mZgEGp83wOmw"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_epi_num=2000, max_epi_len=200):\n",
    "        # capacity is the maximum number of steps in memory\n",
    "        self.max_epi_num = max_epi_num\n",
    "        self.max_epi_len = max_epi_len\n",
    "        # saves each tuple of (state, action, next state, reward)\n",
    "        self.capacity = 1000 # self.max_epi_num * max_epi_len\n",
    "        self.idx = 0\n",
    "        self.obs_memory = np.zeros((self.capacity, 210, 160, 3)) # deque(maxlen=self.max_epi_num * max_epi_len)\n",
    "        self.next_memory = np.zeros((self.capacity, 210, 160, 3))\n",
    "        self.act_memory = np.zeros((self.capacity, 1))\n",
    "        self.reward_memory = np.zeros((self.capacity, 1))\n",
    "        self.is_av = False\n",
    "        self.current_epi = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_epi = 0\n",
    "        self.memory.clear()\n",
    "\n",
    "    ''' deprecated for tuple buffer '''\n",
    "    def create_new_epi(self):\n",
    "        pass\n",
    "\n",
    "    def remember(self, state, next_state, action, reward):\n",
    "        self.obs_memory[self.idx % self.capacity] = state.copy()\n",
    "        self.next_memory[self.idx % self.capacity] = next_state.copy()\n",
    "        self.act_memory[self.idx % self.capacity] = action\n",
    "        self.reward_memory[self.idx % self.capacity] = reward\n",
    "        self.idx += 1\n",
    "        \n",
    "        '''\n",
    "        if len(self.memory) < self.capacity:\n",
    "            new_sample = np.array([state, action, reward, next_state])\n",
    "            if len(self.memory) == 0:\n",
    "                self.memory = [new_sample]\n",
    "            else:\n",
    "                length = len(self.memory)\n",
    "                self.memory.append(new_sample)\n",
    "        '''\n",
    "                \n",
    "    # samples batch_size\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size < self.idx:\n",
    "            max_sz = min(self.capacity, self.idx)\n",
    "            idx = np.random.randint(0, max_sz, batch_size)\n",
    "            return self.obs_memory[idx], self.next_memory[idx], self.act_memory[idx], self.reward_memory[idx]\n",
    "        return self.obs_memory, self.next_memory, self.act_memory, self.reward_memory\n",
    "\n",
    "    def size(self):\n",
    "        return self.idx\n",
    "\n",
    "    def is_available(self):\n",
    "        self.is_av = True\n",
    "        if self.idx <= 1:\n",
    "            self.is_av = False\n",
    "        return self.is_av\n",
    "\n",
    "    def print_info(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "FmR5zvSVwOmw"
   },
   "outputs": [],
   "source": [
    "#@title Create a training conv agent\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQNetworkConv(nn.Module):\n",
    "    def __init__(self, in_channels, act_dim, dueling=False):\n",
    "        super(DQNetworkConv, self).__init__()\n",
    "        self.act_dim = act_dim\n",
    "        self.dueling = dueling\n",
    "        # 160, 210, 3 \n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=5, stride=5)\n",
    "        # Input to conv2: 32, 42, 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # Input to conv3: 15, 20, 64\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=4, stride=1)\n",
    "        # Output from conv3: 12, 17, 64\n",
    "        if self.dueling:\n",
    "            self.v_fc4 = nn.Linear(12 * 17 * 64, 512)\n",
    "            self.adv_fc4 = nn.Linear(12 * 17 * 64, 512)\n",
    "            self.v_fc5 = nn.Linear(512, 1)\n",
    "            self.adv_fc5 = nn.Linear(512, self.act_dim)\n",
    "        else:\n",
    "            self.fc4 = nn.Linear(12 * 17 * 64, 512)\n",
    "            self.fc5 = nn.Linear(512, self.act_dim)\n",
    "        self.params = list(self.conv1.parameters()) + list(self.conv2.parameters()) + list(self.conv3.parameters()) + list(self.fc4.parameters()) + list(self.fc5.parameters())\n",
    "\n",
    "    def forward(self, st):\n",
    "        out = F.relu(self.conv1(st))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.dueling:\n",
    "            val = F.relu(self.v_fc4(out))\n",
    "            adv = F.relu(self.adv_fc4(out))\n",
    "            val = self.v_fc5(val)\n",
    "            adv = self.adv_fc5(adv)\n",
    "            out = val.expand_as(adv) + adv - adv.mean(-1, keepdim=True).expand_as(adv)\n",
    "        else:\n",
    "            out = F.relu(self.fc4(out))\n",
    "            out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "ZGsarcvPzoNm"
   },
   "outputs": [],
   "source": [
    "#@title Create a training FC agent\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQNetworkFC(nn.Module):\n",
    "    def __init__(self, z_dim, act_dim, dueling=False):\n",
    "        super(DQNetworkFC, self).__init__()\n",
    "        self.act_dim = act_dim\n",
    "        self.input_dim = z_dim \n",
    "        self.dueling = dueling\n",
    "        if self.dueling:\n",
    "            self.v_fc1 = nn.Linear(z_dim, 512)\n",
    "            self.adv_fc1 = nn.Linear(z_dim, 512)\n",
    "            self.v_fc2 = nn.Linear(512, 1)\n",
    "            self.adv_fc2 = nn.Linear(512, 256)\n",
    "            self.v_fc3 = nn.Linear(256, 1)\n",
    "            self.adv_fc3 = nn.Linear(256, self.act_dim)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(z_dim, 512)\n",
    "            self.fc2 = nn.Linear(512, 256)\n",
    "            self.fc3 = nn.Linear(256, self.act_dim)\n",
    "\n",
    "    def forward(self, st):\n",
    "        out = F.relu(self.fc1(st))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        ''' Do we need a relu on the last layer if the output is probability over action space? '''\n",
    "        out = F.relu(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "I5dCMf6CH99A"
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "\"\"\" Ignore padding unless using the pretrained imagenet encoder \"\"\"\n",
    "def process_obs(obs, pad=False):\n",
    "    '''\n",
    "    Pad to match the expected encoder input size\n",
    "    '''\n",
    "    obs = torch.Tensor(obs)\n",
    "    if len(obs.shape) < 4:\n",
    "        obs = obs.unsqueeze(0)\n",
    "        \n",
    "    if pad:\n",
    "        w_pad = torch.zeros((obs.shape[0], (224-obs.shape[1])//2, obs.shape[2], 3))\n",
    "        obs = torch.cat((w_pad, obs, w_pad), 1)\n",
    "        h_pad = torch.zeros((obs.shape[0], 224, (224 - obs.shape[2])//2, 3))\n",
    "        obs = torch.cat((h_pad, obs, h_pad), 2)\n",
    "    obs = obs.permute(0, 3, 1, 2)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "H6_wThKowOmx"
   },
   "outputs": [],
   "source": [
    "def take_action(env, action):\n",
    "    state, rew, done, _ = env.step(action)\n",
    "    obs = env.render(mode='rgb_array')\n",
    "    return obs, rew, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "XL9D4MCxwOmx"
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "9NKDLpKGJ2lB"
   },
   "outputs": [],
   "source": [
    "class CURL(nn.Module):\n",
    "    \"\"\"\n",
    "    CURL\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_shape, z_dim, batch_size, encoder, output_type=\"continuous\", critic=None, critic_target=None):\n",
    "        super(CURL, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.encoder = critic.encoder\n",
    "        self.encoder = encoder \n",
    "\n",
    "        # self.encoder_target = critic_target.encoder \n",
    "\n",
    "        self.W = nn.Parameter(torch.rand(z_dim, z_dim))\n",
    "        self.output_type = output_type\n",
    "\n",
    "    def encode(self, x, detach=False, ema=False):\n",
    "        \"\"\"\n",
    "        Encoder: z_t = e(x_t)\n",
    "        :param x: x_t, x y coordinates\n",
    "        :return: z_t, value in r2\n",
    "        \"\"\"\n",
    "        if ema:\n",
    "            with torch.no_grad():\n",
    "                z_out = self.encoder_target(x)\n",
    "        else:\n",
    "            z_out = self.encoder(x)\n",
    "\n",
    "        if detach:\n",
    "            z_out = z_out.detach()\n",
    "        return z_out\n",
    "\n",
    "    def compute_logits(self, z_a, z_mod):\n",
    "        \"\"\"\n",
    "        Uses logits trick for CURL:\n",
    "        - compute (B,B) matrix z_a (W z_pos.T)\n",
    "        - positives are all diagonal elements\n",
    "        - negatives are all other elements\n",
    "        - to compute loss use multiclass cross entropy with identity matrix for labels\n",
    "        \"\"\"\n",
    "        Wz = torch.matmul(self.W, z_mod.T)  # (z_dim,B)\n",
    "        logits = torch.matmul(z_a, Wz)  # (B,B)\n",
    "        logits = logits - torch.max(logits, 1)[0][:, None]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "1wbEY4hTshF6"
   },
   "outputs": [],
   "source": [
    "#@title Generate a batch of negatively labelled examples given observations\n",
    "\n",
    "def generate_negatives(obs):\n",
    "    neg_idx = np.random.randint(len(obs), size=len(obs))\n",
    "    pos_idx = np.arange(len(obs))\n",
    "    resample = (neg_idx == pos_idx)\n",
    "    for (i, r) in enumerate(resample):\n",
    "        if r:\n",
    "            idx = neg_idx[i]\n",
    "        else:\n",
    "            idx = np.random.randint(0, len(obs), 1)[0]\n",
    "            while idx == i:\n",
    "                idx = np.random.randint(0, len(obs), 1)[0]\n",
    "        neg_idx[i] = idx\n",
    "    return (obs[neg_idx]).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "x3KEWmwUwOmx"
   },
   "outputs": [],
   "source": [
    "#@title Create a training agent (wrapper for conv agent)\n",
    "from torch.autograd import Variable\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, act_dim, in_channels=3, max_epi_num=50, max_epi_len=300, encoder=None, conv_net=True, z_dim=50):\n",
    "        self.N_action = act_dim\n",
    "        self.max_epi_num = max_epi_num\n",
    "        self.max_epi_len = max_epi_len\n",
    "        ''' To decide when to copy weights to the target network '''\n",
    "        self.num_param_updates = 0\n",
    "        \n",
    "        ''' Choose between a pre-trained (on Imagenet) encoder or CURL encoder'''\n",
    "        # pretrained encoder --> get its hidden features as the state encoding for RL training\n",
    "        self.encoder = encoder\n",
    "        if conv_net:\n",
    "            self.q_net = DQNetworkConv(in_channels, act_dim)\n",
    "            self.target = DQNetworkConv(in_channels, act_dim)\n",
    "        else:\n",
    "            ''' if using the encoder head for contrastive loss '''\n",
    "            self.q_net = DQNetworkFC(z_dim, act_dim)\n",
    "            self.target = DQNetworkFC(z_dim, act_dim)\n",
    "        self.buffer = ReplayMemory(max_epi_num=self.max_epi_num, max_epi_len=self.max_epi_len)\n",
    "        self.gamma = 0.99\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.params, lr=1e-3)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        ''' Divide by 255 if using resnet or vgg as the pretrained encoder net '''\n",
    "        state = state.copy() / 255.0\n",
    "        next_state = next_state.copy() / 255.0\n",
    "        self.buffer.remember(state, next_state, action, reward)\n",
    "\n",
    "    ''' Copy the weights to the target network every 100 updates '''\n",
    "    def train(self, batch_size=32, target_update_freq=100, use_encoder=False):\n",
    "        if self.buffer.is_available():\n",
    "            obs, next_obs, action_list, reward_list = self.buffer.sample(batch_size)\n",
    "            \n",
    "            ''' Pass through the encoder to get encodings\n",
    "             If also training the contrastive loss\n",
    "             include that here! \n",
    "             1. data augmentation to create pos and negative pairs\n",
    "             2. encoder \n",
    "             3. update encoder loss function (using a separate optimizer) or add to the loss computed below\n",
    "            '''\n",
    "\n",
    "            losses = [] \n",
    "\n",
    "            ''' If not using the encoder, pass the obs directly to the CNN '''\n",
    "            obs = process_obs(obs)\n",
    "            next_obs = process_obs(next_obs)\n",
    "            # estimate current q values from obs\n",
    "            if use_encoder:\n",
    "                z = self.encoder(obs)\n",
    "                next_z = self.encoder(next_obs).detach()\n",
    "            else:\n",
    "                next_z = next_obs\n",
    "                z = obs\n",
    "            Qs = self.q_net(z)\n",
    "            \n",
    "            ''' find target q values ''' \n",
    "            # find next max q values based on next observations\n",
    "            next_qs = self.target(next_z).detach()\n",
    "            next_Qs, max_actions = next_qs.max(1)[0], next_qs.max(1)[1]\n",
    "            \n",
    "            '''\n",
    "            # create a mask for actions taken in the env\n",
    "            mask = torch.zeros((Qs.shape))\n",
    "            for (i, action) in enumerate(action_list):\n",
    "                mask[i, int(action[0])] = 1.0\n",
    "            mask = Variable(mask, requires_grad=True)\n",
    "            # Qs = torch.gather(Qs, dim=1, index=torch.tensor(action_list, dtype=torch.int64)).reshape(-1)\n",
    "            \n",
    "            next_Qs = next_Qs.numpy()\n",
    "            target_qs = torch.tensor(reward_list.squeeze(-1) + GAMMA * next_Qs).long()\n",
    "            # try to set Qs equal to target_Qs \n",
    "            Qs = Qs * mask\n",
    "            target_Qs = torch.zeros((Qs.shape))\n",
    "            for (i, q) in enumerate(target_qs):\n",
    "                target_Qs[i, int(action_list[i][0])] = q\n",
    "            '''\n",
    "            \n",
    "            next_Qs = next_Qs.numpy()\n",
    "            \n",
    "            target_qs = torch.tensor(reward_list.squeeze(-1) + GAMMA * next_Qs).float()\n",
    "            target_qs = target_qs.repeat(18).reshape(18, 32)\n",
    "            target_qs = target_qs.permute(1, 0)\n",
    "            # Qs.requires_grad = True\n",
    "            q_loss = torch.sum((Qs - target_qs) ** 2)\n",
    "            # q_loss = self.loss_fn(Qs, target_qs).long()\n",
    "            losses.append(q_loss)\n",
    "            \n",
    "            ''' Loss update for q network and encoder head '''\n",
    "            losses = torch.stack(losses).sum()\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.num_param_updates += 1\n",
    "            if self.num_param_updates % target_update_freq == 0:\n",
    "                self.target.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    # TODO: check the sizes of inputs and outputs\n",
    "    def get_action(self, obs, epsilon, use_encoding=False):\n",
    "        ''' \n",
    "         If using an encoder, need to pass that thorugh the encoder\n",
    "         then use the encoding to pass through self.conv_net\n",
    "        '''\n",
    "        obs = process_obs(obs)\n",
    "\n",
    "        # Dividing obs by 255 is handled in encoder forward (only needed for use_encoding=False)\n",
    "        if len(obs.shape) == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        # epsilon greedy for selecting which action to take\n",
    "        if random.random() > epsilon:\n",
    "            if use_encoding:\n",
    "                zs = self.encoder(obs).detach()\n",
    "            else:\n",
    "                zs = obs\n",
    "            qs = self.q_net(zs)\n",
    "            action = qs[0].argmax().data.item()\n",
    "        else:\n",
    "            action = random.randint(0, self.N_action-1)\n",
    "\n",
    "        return action\n",
    "\n",
    "def get_decay(epi_iter):\n",
    "    decay = math.pow(0.999, epi_iter)\n",
    "    if decay < 0.05:\n",
    "        decay = 0.05\n",
    "    return decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Replace CURL encoder with VGG11 encoder head \"\"\"\n",
    "import torch\n",
    "# vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_dim = 4096 # for vgg output before num classes\n",
    "# for param in vgg.parameters():\n",
    "#    param.requires_grad = False\n",
    "# vgg.classifier[-1] = nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "1WWwX8wewOmy"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    if not os.path.exists(\"exps\"):\n",
    "        os.makedirs(\"exps\")\n",
    "        \n",
    "    env = gym.make('ALE/SpaceInvaders-v5')\n",
    "    max_epi_iter = 4000\n",
    "    max_MC_iter = 400\n",
    "    obs = env.render(mode='rgb_array')\n",
    "    agent = Agent(act_dim=env.action_space.n, max_epi_num=10000, max_epi_len=max_MC_iter)\n",
    "    train_curve = []\n",
    "    for epi_iter in range(max_epi_iter):\n",
    "        random.seed()\n",
    "        env.reset()\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        returns = 0.0\n",
    "        for MC_iter in range(max_MC_iter):\n",
    "            # Make sure to set this to an actual action\n",
    "            # TODO: decide whether to normalize pixels by / 255 (already done in PixelEncoder atm)\n",
    "            action = agent.get_action(obs, get_decay(epi_iter))\n",
    "            next_obs, reward, done = take_action(env, action)\n",
    "            \n",
    "            returns += reward * agent.gamma ** (MC_iter)\n",
    "          \n",
    "            agent.remember(obs, action, reward, next_obs)\n",
    "            obs = next_obs.copy()\n",
    "            if done or MC_iter >= max_MC_iter-1:\n",
    "                agent.buffer.create_new_epi()\n",
    "                break\n",
    "        print('Episode', epi_iter, 'returns', returns)\n",
    "        if epi_iter % 1 == 0:\n",
    "            train_curve.append(returns)\n",
    "        if epi_iter % 20 == 0:\n",
    "            print(\"saving results from {}\".format(epi_iter))\n",
    "            np.save(\"exps/pixels_rew.npy\", np.array(train_curve))\n",
    "        if agent.buffer.is_available():\n",
    "            for i in range(5):\n",
    "                agent.train()\n",
    "        \n",
    "    print(train_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 returns 27.194415194784664\n",
      "saving results from 0\n",
      "Episode 1 returns 24.889081142717618\n",
      "Episode 2 returns 23.806823431871063\n",
      "Episode 3 returns 35.49832542936917\n",
      "Episode 4 returns 32.27889277746253\n",
      "Episode 5 returns 14.049587438953521\n",
      "Episode 6 returns 34.44705198981342\n",
      "Episode 7 returns 5.704675574314388\n",
      "Episode 8 returns 4.46319035034535\n",
      "Episode 9 returns 24.576542046828617\n",
      "Episode 10 returns 21.662601668210225\n",
      "Episode 11 returns 27.044171471616096\n",
      "Episode 12 returns 15.56118586861987\n",
      "Episode 13 returns 9.372667299495376\n",
      "Episode 14 returns 8.182453858786069\n",
      "Episode 15 returns 11.768560266272276\n",
      "Episode 16 returns 26.148593964268333\n",
      "Episode 17 returns 9.111776890502105\n",
      "Episode 18 returns 27.92572103936982\n",
      "Episode 19 returns 25.115706607494864\n",
      "Episode 20 returns 3.607426761535656\n",
      "saving results from 20\n",
      "Episode 21 returns 41.56257712442804\n",
      "Episode 22 returns 12.88518071378746\n",
      "Episode 23 returns 28.060736343394815\n",
      "Episode 24 returns 32.920631897127954\n",
      "Episode 25 returns 25.31040077662757\n",
      "Episode 26 returns 21.021535794101883\n",
      "Episode 27 returns 30.15984452558824\n",
      "Episode 28 returns 42.13052079104408\n",
      "Episode 29 returns 35.72742903945097\n",
      "Episode 30 returns 17.915048656152397\n",
      "Episode 31 returns 17.082709208259917\n",
      "Episode 32 returns 30.237230826477713\n",
      "Episode 33 returns 1.0424623086738063\n",
      "Episode 34 returns 9.337824599660205\n",
      "Episode 35 returns 20.07579303803235\n",
      "Episode 36 returns 23.74429401534671\n",
      "Episode 37 returns 17.061652025601024\n",
      "Episode 38 returns 18.47552725759783\n",
      "Episode 39 returns 20.433380216975113\n",
      "Episode 40 returns 18.283078998292172\n",
      "saving results from 40\n",
      "Episode 41 returns 23.496540111854205\n",
      "Episode 42 returns 22.176013877391632\n",
      "Episode 43 returns 15.977266190804647\n",
      "Episode 44 returns 19.83390098608619\n",
      "Episode 45 returns 28.125544795540808\n",
      "Episode 46 returns 17.930755093427617\n",
      "Episode 47 returns 4.082281392294096\n",
      "Episode 48 returns 14.29425231697529\n",
      "Episode 49 returns 32.581188077974794\n",
      "Episode 50 returns 20.38071517168109\n",
      "Episode 51 returns 37.76468935611601\n",
      "Episode 52 returns 24.613416873033653\n",
      "Episode 53 returns 16.4105712557903\n",
      "Episode 54 returns 6.924577737365181\n",
      "Episode 55 returns 30.34285545072308\n",
      "Episode 56 returns 44.62808919402984\n",
      "Episode 57 returns 8.44726103990573\n",
      "Episode 58 returns 15.993169254221648\n",
      "Episode 59 returns 5.486519360585203\n",
      "Episode 60 returns 26.954346025030226\n",
      "saving results from 60\n",
      "Episode 61 returns 26.163037507176536\n",
      "Episode 62 returns 15.780089527538102\n",
      "Episode 63 returns 12.952892444624908\n",
      "Episode 64 returns 20.844845896476386\n",
      "Episode 65 returns 12.155376808362357\n",
      "Episode 66 returns 48.96867303362308\n",
      "Episode 67 returns 26.08207458441004\n",
      "Episode 68 returns 20.159331763864152\n",
      "Episode 69 returns 31.90363974546746\n",
      "Episode 70 returns 24.580043002255685\n",
      "Episode 71 returns 36.64801326364845\n",
      "Episode 72 returns 25.984339889628952\n",
      "Episode 73 returns 35.97269934584808\n",
      "Episode 74 returns 28.358936602574516\n",
      "Episode 75 returns 37.2154712232011\n",
      "Episode 76 returns 25.395970467518698\n",
      "Episode 77 returns 25.06486192484503\n",
      "Episode 78 returns 6.286456777351622\n",
      "Episode 79 returns 37.22218211204574\n",
      "Episode 80 returns 34.046139627179116\n",
      "saving results from 80\n",
      "Episode 81 returns 24.293427744256814\n",
      "Episode 82 returns 25.022642744725285\n",
      "Episode 83 returns 7.5385649812577675\n",
      "Episode 84 returns 22.780336670613398\n",
      "Episode 85 returns 15.793976749803631\n",
      "Episode 86 returns 13.682009071357395\n",
      "Episode 87 returns 22.57488648238077\n",
      "Episode 88 returns 27.6256751070971\n",
      "Episode 89 returns 16.44686316845325\n",
      "Episode 90 returns 14.980960469369451\n",
      "Episode 91 returns 25.75131605083358\n",
      "Episode 92 returns 37.85491023317244\n",
      "Episode 93 returns 33.5227161409071\n",
      "Episode 94 returns 13.131511500950758\n",
      "Episode 95 returns 23.455520106019915\n",
      "Episode 96 returns 44.921669813809864\n",
      "Episode 97 returns 19.067736134884335\n",
      "Episode 98 returns 13.12778133165887\n",
      "Episode 99 returns 24.421627290004636\n",
      "Episode 100 returns 23.340352734015156\n",
      "saving results from 100\n",
      "Episode 101 returns 25.622701778803304\n",
      "Episode 102 returns 12.526911455862413\n",
      "Episode 103 returns 14.58572390664109\n",
      "Episode 104 returns 52.68088008955542\n",
      "Episode 105 returns 39.27404013334456\n",
      "Episode 106 returns 23.33417138710567\n",
      "Episode 107 returns 19.980677511289873\n",
      "Episode 108 returns 24.479076514190055\n",
      "Episode 109 returns 39.953750404755596\n",
      "Episode 110 returns 37.68731475278056\n",
      "Episode 111 returns 18.484521819287664\n",
      "Episode 112 returns 11.294175506262558\n",
      "Episode 113 returns 10.345091024066475\n",
      "Episode 114 returns 29.705660439093233\n",
      "Episode 115 returns 20.523921237219398\n",
      "Episode 116 returns 9.28473657609594\n",
      "Episode 117 returns 19.02806869943325\n",
      "Episode 118 returns 24.18997433609964\n",
      "Episode 119 returns 20.606163364907445\n",
      "Episode 120 returns 16.659471297877285\n",
      "saving results from 120\n",
      "Episode 121 returns 20.692167686195294\n",
      "Episode 122 returns 13.092974584256087\n",
      "Episode 123 returns 24.607961212683506\n",
      "Episode 124 returns 29.746525646294177\n",
      "Episode 125 returns 24.411901726324924\n",
      "Episode 126 returns 23.402500592508254\n",
      "Episode 127 returns 2.9433726135346543\n",
      "Episode 128 returns 37.85286497230863\n",
      "Episode 129 returns 19.830704712248902\n",
      "Episode 130 returns 23.137174831414775\n",
      "Episode 131 returns 14.636932430493044\n",
      "Episode 132 returns 6.648434076893128\n",
      "Episode 133 returns 5.772952229327188\n",
      "Episode 134 returns 23.964356191176652\n",
      "Episode 135 returns 20.158702263107653\n",
      "Episode 136 returns 44.372819053792675\n",
      "Episode 137 returns 44.84347730024646\n",
      "Episode 138 returns 33.047438943833555\n",
      "Episode 139 returns 5.770487460061025\n",
      "Episode 140 returns 25.963602040836303\n",
      "saving results from 140\n",
      "Episode 141 returns 28.501281296204986\n",
      "Episode 142 returns 30.13978347096748\n",
      "Episode 143 returns 20.900756709597285\n",
      "Episode 144 returns 9.434040610607969\n",
      "Episode 145 returns 18.920247494199987\n",
      "Episode 146 returns 28.958399904392348\n",
      "Episode 147 returns 27.211752627220008\n",
      "Episode 148 returns 16.396782614300804\n",
      "Episode 149 returns 19.920105185683315\n",
      "Episode 150 returns 35.70120897029754\n",
      "Episode 151 returns 14.983707734359303\n",
      "Episode 152 returns 9.788343771512215\n",
      "Episode 153 returns 14.97346515579109\n",
      "Episode 154 returns 33.54343151131527\n",
      "Episode 155 returns 20.941307345496845\n",
      "Episode 156 returns 24.072083841597152\n",
      "Episode 157 returns 19.21587326923917\n",
      "Episode 158 returns 45.873533219515835\n",
      "Episode 159 returns 8.985495395505973\n",
      "Episode 160 returns 14.754456358978068\n",
      "saving results from 160\n",
      "Episode 161 returns 4.324136492566309\n",
      "Episode 162 returns 22.04781929121109\n",
      "Episode 163 returns 30.18692589044968\n",
      "Episode 164 returns 15.68955742614788\n",
      "Episode 165 returns 31.777600192950988\n",
      "Episode 166 returns 16.24974995508246\n",
      "Episode 167 returns 62.297566799756034\n",
      "Episode 168 returns 17.890967366543897\n",
      "Episode 169 returns 28.70753311136111\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkCfk2GjBNRl"
   },
   "source": [
    "Code references for DQN:\n",
    "\n",
    "https://github.com/taochenshh/dqn-pytorch\n",
    "\n",
    "https://github.com/transedward/pytorch-dqn (for sampling from replay buffer)\n",
    "\n",
    "CURL code: https://github.com/MishaLaskin/curl\n",
    "\n",
    "Code reference for using a pretrained network on imagenet (VGG):\n",
    "https://pytorch.org/hub/pytorch_vision_vgg/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sample_gym_env_v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
