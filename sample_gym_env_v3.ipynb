{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h6v4c9_UwOmr",
    "outputId": "d5b5140a-cd61-45f6-859d-79986d112476"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import math\n",
    "import gym \n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import warnings\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "from collections import Counter, deque\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm.notebook import tqdm\n",
    "from gym.wrappers import Monitor\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "import io\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import kornia.augmentation as aug\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XREqct6awOms"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7fa730025f70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=False, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T1ACavBgwOmt"
   },
   "outputs": [],
   "source": [
    "import os, sys, copy, argparse, shutil\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Deep Q Network Argument Parser')\n",
    "    parser.add_argument('--seed', dest='seed', type=int, default=1)\n",
    "    parser.add_argument('--env', dest='env', type=str, default='CartPole-v0')\n",
    "    parser.add_argument('--save_interval', type=int, default=50, help='save model every n episodes')\n",
    "    parser.add_argument('--log_interval', type=int, default=10, help='logging every n episodes')\n",
    "    parser.add_argument('--render', help='render', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', help='batch_size', type=int, default=32)\n",
    "    parser.add_argument('--train_freq', help='train_frequency', type=int, default=1)\n",
    "    parser.add_argument('--max_episode', help='maximum episode', type=int, default=None)\n",
    "    parser.add_argument('--max_timesteps', help='maximum timestep', type=int, default=100000000)\n",
    "    parser.add_argument('--lr', dest='lr', type=float, default=0.00025)\n",
    "    parser.add_argument('--lr_decay', action='store_true', help='decay learning rate')\n",
    "    parser.add_argument('--gamma', help='discount_factor', type=float, default=0.99)\n",
    "    parser.add_argument('--warmup_mem', type=int, help='warmup memory size', default=1000)\n",
    "    parser.add_argument('--frame_skip', type=int, help='number of frames to skip for each action', default=3)\n",
    "    parser.add_argument('--frame_stack', type=int, help='number of frames to stack', default=4)\n",
    "    parser.add_argument('--memory', help='memory size', type=int, default=1000000)\n",
    "    parser.add_argument('--initial_epsilon', '-ie', help='initial_epsilon', type=float, default=0.5)\n",
    "    parser.add_argument('--final_epsilon', '-fe', help='final_epsilon', type=float, default=0.05)\n",
    "    parser.add_argument('--max_epsilon_decay_steps', '-eds', help='maximum steps to decay epsilon', type=int, default=100000)\n",
    "    parser.add_argument('--max_grad_norm', type=float, default=None, help='maximum gradient norm')\n",
    "    parser.add_argument('--soft_update', '-su', action='store_true', help='soft update target network')\n",
    "    parser.add_argument('--double_q', '-dq', action='store_true', help='enabling double DQN')\n",
    "    parser.add_argument('--dueling_net', '-dn', action='store_true', help='enabling dueling network')\n",
    "    parser.add_argument('--test', action='store_true', help='test the trained model')\n",
    "    parser.add_argument('--tau', type=float, default=0.01, help='tau for soft target network update')\n",
    "    parser.add_argument('--hard_update_freq', '-huf', type=int, default=2000, help='hard target network update frequency')\n",
    "    parser.add_argument('--save_dir', type=str, default='./data')\n",
    "    parser.add_argument('--resume_step', '-rs', type=int, default=None)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X3Rkxe4VwOmu"
   },
   "outputs": [],
   "source": [
    "#@title Set up constants for env and training\n",
    "test = False \n",
    "save_dir = './data'\n",
    "render = False\n",
    "max_episode = None\n",
    "max_timesteps = 100000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "539HMtWFwOmv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolor_jitter\\nrandom_elastic_transform\\nrandom_fisheye\\nrandom_color_equalize\\nrandom_gaussian_blur\\nrandom_gaussian_noise\\nrandom_horizontal_flip\\nrandom_color_invert\\nrandom_perspective_shift\\nrandom_shift\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Augmentations\n",
    "'''\n",
    "color_jitter\n",
    "random_elastic_transform\n",
    "random_fisheye\n",
    "random_color_equalize\n",
    "random_gaussian_blur\n",
    "random_gaussian_noise\n",
    "random_horizontal_flip\n",
    "random_color_invert\n",
    "random_perspective_shift\n",
    "random_shift\n",
    "'''\n",
    "color_jitter = aug.ColorJitter(\n",
    "        brightness=np.random.random(),\n",
    "        contrast=np.random.random(),\n",
    "        saturation=np.random.random(),\n",
    "        hue=np.random.random(),\n",
    "        p=0.5\n",
    "        )\n",
    "random_elastic_transform = aug.RandomElasticTransform()\n",
    "random_fisheye = aug.RandomFisheye(\n",
    "        center_x=torch.tensor([-.3, .3]).to(device),\n",
    "        center_y=torch.tensor([-.3, .3]).to(device),\n",
    "        gamma=torch.tensor([.9, 1.]).to(device),\n",
    "        )\n",
    "# need to divide by 255.0\n",
    "random_color_equalize = aug.RandomEqualize()\n",
    "random_gaussian_blur = aug.RandomGaussianBlur(\n",
    "        kernel_size=(9, 9),\n",
    "        sigma = (5., 5.)\n",
    "        )\n",
    "random_gaussian_noise = aug.RandomGaussianNoise()\n",
    "random_horizontal_flip = aug.RandomHorizontalFlip()\n",
    "random_color_invert = aug.RandomInvert()\n",
    "random_perspective_shift = aug.RandomPerspective()\n",
    "random_shift = nn.Sequential(aug.RandomCrop((190, 140)), nn.ReplicationPad2d(20), aug.RandomCrop((210, 160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K78NVCz-oqHc"
   },
   "outputs": [],
   "source": [
    "def tie_weights(src, trg):\n",
    "    assert type(src) == type(trg)\n",
    "    trg.weight = src.weight\n",
    "    trg.bias = src.bias\n",
    "\n",
    "\n",
    "# for 84 x 84 inputs\n",
    "OUT_DIM = {2: 39, 4: 35, 6: 31}\n",
    "# for 64 x 64 inputs\n",
    "OUT_DIM_64 = {2: 29, 4: 25, 6: 21}\n",
    "\n",
    "''' TODO change the layer parameters ''' \n",
    "class PixelEncoder(nn.Module):\n",
    "    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n",
    "    def __init__(self, obs_shape, feature_dim=50, num_layers=3, num_filters=64, output_logits=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(obs_shape) == 3\n",
    "        self.obs_shape = obs_shape\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 160, 210, 3 \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=5)\n",
    "        # Input to conv2: 32, 42, 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # Input to conv3: 15, 20, 64\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=4, stride=1)\n",
    "        # Output from conv3: 12, 17, 64\n",
    "\n",
    "        # out_dim = OUT_DIM_64[num_layers] if obs_shape[-1] == 64 else OUT_DIM[num_layers]\n",
    "        out_dims = (12, 17)\n",
    "        self.fc = nn.Linear(num_filters * out_dims[0] * out_dims[1], self.feature_dim)\n",
    "        self.ln = nn.LayerNorm(self.feature_dim)\n",
    "\n",
    "        self.outputs = dict()\n",
    "        self.output_logits = output_logits\n",
    "\n",
    "    def reparameterize(self, mu, logstd):\n",
    "        std = torch.exp(logstd)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward_conv(self, obs):\n",
    "        self.outputs['obs'] = obs\n",
    "        conv1 = torch.relu(self.conv1(obs))\n",
    "        self.outputs['conv1'] = conv1\n",
    "        conv2 = torch.relu(self.conv2(conv1))\n",
    "        self.outputs['conv2'] = conv2\n",
    "        conv3 = torch.relu(self.conv3(conv2))\n",
    "        self.outputs['conv3'] = conv3\n",
    "\n",
    "        h = conv3.reshape(conv3.size(0), -1)\n",
    "        return h\n",
    "\n",
    "    def forward(self, obs, detach=False):\n",
    "        h = self.forward_conv(obs)\n",
    "\n",
    "        if detach:\n",
    "            h = h.detach()\n",
    "\n",
    "        h_fc = self.fc(h)\n",
    "        self.outputs['fc'] = h_fc\n",
    "\n",
    "        h_norm = self.ln(h_fc)\n",
    "        self.outputs['ln'] = h_norm\n",
    "\n",
    "        if self.output_logits:\n",
    "            out = h_norm\n",
    "        else:\n",
    "            out = torch.tanh(h_norm)\n",
    "            self.outputs['tanh'] = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def copy_conv_weights_from(self, source):\n",
    "        \"\"\"Tie convolutional layers\"\"\"\n",
    "        # only tie conv layers\n",
    "        for i in range(self.num_layers):\n",
    "            tie_weights(src=source.convs[i], trg=self.convs[i])\n",
    "\n",
    "    def log(self, L, step, log_freq):\n",
    "        if step % log_freq != 0:\n",
    "            return\n",
    "\n",
    "        for k, v in self.outputs.items():\n",
    "            L.log_histogram('train_encoder/%s_hist' % k, v, step)\n",
    "            if len(v.shape) > 2:\n",
    "                L.log_image('train_encoder/%s_img' % k, v[0], step)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            L.log_param('train_encoder/conv%s' % (i + 1), self.convs[i], step)\n",
    "        L.log_param('train_encoder/fc', self.fc, step)\n",
    "        L.log_param('train_encoder/ln', self.ln, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0kIzyV1QwOmw"
   },
   "outputs": [],
   "source": [
    "def wrap_env(env, train=True):\n",
    "    suffix = 'train' if train else 'test'\n",
    "    monitor_dir = os.path.join(save_dir, 'monitor_%s' % suffix)\n",
    "    os.makedirs(monitor_dir, exist_ok=True)\n",
    "    if not train:\n",
    "        video_save_interval = 10\n",
    "        env = Monitor(env, directory=monitor_dir,\n",
    "                      video_callable=lambda episode_id: episode_id % video_save_interval == 0,\n",
    "                      force=True)\n",
    "    else:\n",
    "        if render:\n",
    "            if max_episode is not None:\n",
    "                video_save_interval = int(max_episode / 3)\n",
    "            else:\n",
    "                video_save_interval = int(max_timesteps / float(env._max_episode_steps) / 3)\n",
    "            env = Monitor(env, directory=monitor_dir,\n",
    "                          video_callable=lambda episode_id: episode_id % video_save_interval == 0,\n",
    "                          force=True)\n",
    "        else:\n",
    "            env = Monitor(env, directory=monitor_dir, video_callable=False, force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mZgEGp83wOmw"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_epi_num=2000, max_epi_len=200):\n",
    "        # capacity is the maximum number of steps in memory\n",
    "        self.max_epi_num = max_epi_num\n",
    "        self.max_epi_len = max_epi_len\n",
    "        # saves each tuple of (state, action, next state, reward)\n",
    "        self.capacity = 1000 # self.max_epi_num * max_epi_len\n",
    "        self.idx = 0\n",
    "        self.obs_memory = np.zeros((self.capacity, 210, 160, 3)) # deque(maxlen=self.max_epi_num * max_epi_len)\n",
    "        self.next_memory = np.zeros((self.capacity, 210, 160, 3))\n",
    "        self.act_memory = np.zeros((self.capacity, 1))\n",
    "        self.reward_memory = np.zeros((self.capacity, 1))\n",
    "        self.is_av = False\n",
    "        self.current_epi = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_epi = 0\n",
    "        self.memory.clear()\n",
    "\n",
    "    ''' deprecated for tuple buffer '''\n",
    "    def create_new_epi(self):\n",
    "        pass\n",
    "\n",
    "    def remember(self, state, next_state, action, reward):\n",
    "        if self.idx == self.capacity:\n",
    "            self.idx = 0\n",
    "        self.obs_memory[self.idx] = state.copy()\n",
    "        self.next_memory[self.idx] = next_state.copy()\n",
    "        self.act_memory[self.idx] = action\n",
    "        self.reward_memory[self.idx] = reward\n",
    "        self.idx += 1\n",
    "        \n",
    "        '''\n",
    "        if len(self.memory) < self.capacity:\n",
    "            new_sample = np.array([state, action, reward, next_state])\n",
    "            if len(self.memory) == 0:\n",
    "                self.memory = [new_sample]\n",
    "            else:\n",
    "                length = len(self.memory)\n",
    "                self.memory.append(new_sample)\n",
    "        '''\n",
    "                \n",
    "    # samples batch_size\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size < self.idx:\n",
    "            idx = np.random.randint(0, self.idx - 1, batch_size)\n",
    "            return self.obs_memory[idx], self.next_memory[idx], self.act_memory[idx], self.reward_memory[idx]\n",
    "        return self.obs_memory, self.next_memory, self.act_memory, self.reward_memory\n",
    "\n",
    "    def size(self):\n",
    "        return self.idx\n",
    "\n",
    "    def is_available(self):\n",
    "        self.is_av = True\n",
    "        if self.idx <= 1:\n",
    "            self.is_av = False\n",
    "        return self.is_av\n",
    "\n",
    "    def print_info(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FmR5zvSVwOmw"
   },
   "outputs": [],
   "source": [
    "#@title Create a training conv agent\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQNetworkConv(nn.Module):\n",
    "    def __init__(self, in_channels, act_dim, dueling=False):\n",
    "        super(DQNetworkConv, self).__init__()\n",
    "        self.act_dim = act_dim\n",
    "        self.dueling = dueling\n",
    "        # 160, 210, 3 \n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=5, stride=5)\n",
    "        # Input to conv2: 32, 42, 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # Input to conv3: 15, 20, 64\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=4, stride=1)\n",
    "        # Output from conv3: 12, 17, 64\n",
    "        if self.dueling:\n",
    "            self.v_fc4 = nn.Linear(12 * 17 * 64, 512)\n",
    "            self.adv_fc4 = nn.Linear(12 * 17 * 64, 512)\n",
    "            self.v_fc5 = nn.Linear(512, 1)\n",
    "            self.adv_fc5 = nn.Linear(512, self.act_dim)\n",
    "        else:\n",
    "            self.fc4 = nn.Linear(12 * 17 * 64, 512)\n",
    "            self.fc5 = nn.Linear(512, self.act_dim)\n",
    "        self.parameters = (list(self.conv1.parameters())) + (list(self.conv2.parameters())) + (list(self.conv3.parameters())) + (list(self.fc4.parameters())) + (list(self.fc5.parameters()))\n",
    "\n",
    "    def forward(self, st):\n",
    "        out = F.relu(self.conv1(st))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if self.dueling:\n",
    "            val = F.relu(self.v_fc4(out))\n",
    "            adv = F.relu(self.adv_fc4(out))\n",
    "            val = self.v_fc5(val)\n",
    "            adv = self.adv_fc5(adv)\n",
    "            out = val.expand_as(adv) + adv - adv.mean(-1, keepdim=True).expand_as(adv)\n",
    "        else:\n",
    "            out = F.relu(self.fc4(out))\n",
    "            out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZGsarcvPzoNm"
   },
   "outputs": [],
   "source": [
    "#@title Create a training FC agent\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQNetworkFC(nn.Module):\n",
    "    def __init__(self, z_dim, act_dim, dueling=False):\n",
    "        super(DQNetworkFC, self).__init__()\n",
    "        self.act_dim = act_dim\n",
    "        self.input_dim = z_dim \n",
    "        self.dueling = dueling\n",
    "        if self.dueling:\n",
    "            self.v_fc1 = nn.Linear(z_dim, 512)\n",
    "            self.adv_fc1 = nn.Linear(z_dim, 512)\n",
    "            self.v_fc2 = nn.Linear(512, 1)\n",
    "            self.adv_fc2 = nn.Linear(512, 256)\n",
    "            self.v_fc3 = nn.Linear(256, 1)\n",
    "            self.adv_fc3 = nn.Linear(256, self.act_dim)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(z_dim, 512)\n",
    "            self.fc2 = nn.Linear(512, 256)\n",
    "            self.fc3 = nn.Linear(256, self.act_dim)\n",
    "\n",
    "    def forward(self, st):\n",
    "        out = F.relu(self.fc1(st))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        ''' Do we need a relu on the last layer if the output is probability over action space? '''\n",
    "        out = F.relu(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I5dCMf6CH99A"
   },
   "outputs": [],
   "source": [
    "def process_obs(obs):\n",
    "    obs = torch.Tensor(obs / 255.)\n",
    "    if len(obs.shape) < 4:\n",
    "        obs = obs.unsqueeze(0)\n",
    "    obs = obs.permute(0, 3, 1, 2)\n",
    "    return obs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H6_wThKowOmx"
   },
   "outputs": [],
   "source": [
    "def take_action(env, action):\n",
    "    state, rew, done, _ = env.step(action)\n",
    "    obs = env.render(mode='rgb_array')\n",
    "    return obs, rew, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XL9D4MCxwOmx"
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9NKDLpKGJ2lB"
   },
   "outputs": [],
   "source": [
    "class CURL(nn.Module):\n",
    "    \"\"\"\n",
    "    CURL\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_shape, z_dim, batch_size, encoder, output_type=\"continuous\", critic=None, critic_target=None):\n",
    "        super(CURL, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.encoder = critic.encoder\n",
    "        self.encoder = encoder \n",
    "\n",
    "        # self.encoder_target = critic_target.encoder \n",
    "\n",
    "        self.W = nn.Parameter(torch.rand(z_dim, z_dim))\n",
    "        self.output_type = output_type\n",
    "\n",
    "    def encode(self, x, detach=False, ema=False):\n",
    "        \"\"\"\n",
    "        Encoder: z_t = e(x_t)\n",
    "        :param x: x_t, x y coordinates\n",
    "        :return: z_t, value in r2\n",
    "        \"\"\"\n",
    "        if ema:\n",
    "            with torch.no_grad():\n",
    "                z_out = self.encoder_target(x)\n",
    "        else:\n",
    "            z_out = self.encoder(x)\n",
    "\n",
    "        if detach:\n",
    "            z_out = z_out.detach()\n",
    "        return z_out\n",
    "\n",
    "    def compute_logits(self, z_a, z_mod):\n",
    "        \"\"\"\n",
    "        Uses logits trick for CURL:\n",
    "        - compute (B,B) matrix z_a (W z_pos.T)\n",
    "        - positives are all diagonal elements\n",
    "        - negatives are all other elements\n",
    "        - to compute loss use multiclass cross entropy with identity matrix for labels\n",
    "        \"\"\"\n",
    "        Wz = torch.matmul(self.W, z_mod.T)  # (z_dim,B)\n",
    "        logits = torch.matmul(z_a, Wz)  # (B,B)\n",
    "        logits = logits - torch.max(logits, 1)[0][:, None]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1wbEY4hTshF6"
   },
   "outputs": [],
   "source": [
    "#@title Generate a batch of negatively labelled examples given observations\n",
    "\n",
    "def generate_negatives(obs):\n",
    "    neg_idx = np.random.randint(len(obs), size=len(obs))\n",
    "    pos_idx = np.arange(len(obs))\n",
    "    resample = (neg_idx == pos_idx)\n",
    "    for (i, r) in enumerate(resample):\n",
    "        if r:\n",
    "            idx = neg_idx[i]\n",
    "        else:\n",
    "            idx = np.random.randint(0, len(obs), 1)[0]\n",
    "            while idx == i:\n",
    "                idx = np.random.randint(0, len(obs), 1)[0]\n",
    "        neg_idx[i] = idx\n",
    "    return (obs[neg_idx]).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "x3KEWmwUwOmx"
   },
   "outputs": [],
   "source": [
    "#@title Create a training agent (wrapper for conv agent)\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, act_dim, in_channels=3, max_epi_num=50, max_epi_len=300, CURL=None, aug=random_shift, conv_net=False):\n",
    "        self.N_action = act_dim\n",
    "        self.max_epi_num = max_epi_num\n",
    "        self.max_epi_len = max_epi_len\n",
    "        ''' To decide when to copy weights to the target network '''\n",
    "        self.num_param_updates = 0\n",
    "        self.CURL = CURL\n",
    "        self.aug = aug\n",
    "        if conv_net:\n",
    "            self.conv_net = DQNetworkConv(in_channels, act_dim).to(device)\n",
    "            self.target = DQNetworkConv(in_channels, act_dim).to(device)\n",
    "        else:\n",
    "            ''' if using the encoder head for contrastive loss '''\n",
    "            self.conv_net = DQNetworkFC(self.CURL.encoder.feature_dim, act_dim).to(device)\n",
    "            self.target = DQNetworkFC(self.CURL.encoder.feature_dim, act_dim).to(device)\n",
    "        self.buffer = ReplayMemory(max_epi_num=self.max_epi_num, max_epi_len=self.max_epi_len)\n",
    "        self.gamma = 0.99\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            list(self.conv_net.parameters()) + \n",
    "            list(self.CURL.parameters()) + \n",
    "            list(self.CURL.encoder.parameters()), lr=1e-3)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.buffer.remember(state, next_state, action, reward)\n",
    "\n",
    "    ''' Copy the weights to the target network every 100 updates '''\n",
    "    def train(self, batch_size=32, target_update_freq=100, use_encoder=True):\n",
    "        if self.buffer.is_available():\n",
    "            obs, next_obs, action_list, reward_list = self.buffer.sample(batch_size)\n",
    "            \n",
    "            ''' Pass through the encoder to get encodings\n",
    "             If also training the contrastive loss\n",
    "             include that here! \n",
    "             1. data augmentation to create pos and negative pairs\n",
    "             2. encoder \n",
    "             3. update encoder loss function (using a separate optimizer) or add to the loss computed below\n",
    "            '''\n",
    "\n",
    "            losses = [] \n",
    "\n",
    "            # check if obs is a numpy or a torch tensor\n",
    "            if use_encoder:\n",
    "                obs_anchor = process_obs(obs.copy()) # / 255.)\n",
    "                if self.aug is not None:\n",
    "                    obs_pos = self.aug(process_obs(obs.copy()))\n",
    "                else:\n",
    "                    obs_pos = process_obs(obs.copy())\n",
    "                # mixed_obs = generate_negatives(obs)\n",
    "                # mixed_obs = process_obs(mixed_obs)\n",
    "                # obs_neg = random_shift(mixed_obs)\n",
    "                z_a = self.CURL.encode(obs_anchor)\n",
    "                z_pos = self.CURL.encode(obs_pos)\n",
    "                # Mix pairs to generate negative labels\n",
    "                # z_neg = self.CURL.encode(obs_neg)\n",
    "                next_obs = process_obs(next_obs.copy())\n",
    "                z_next = self.CURL.encode(next_obs)\n",
    "\n",
    "                logits = self.CURL.compute_logits(z_a, z_pos)\n",
    "                labels = torch.arange(logits.shape[0]).long().to(device)\n",
    "                '''\n",
    "                pos_logits = self.CURL.compute_logits(z_a, z_pos)\n",
    "                neg_logits = self.CURL.compute_logits(z_a, z_neg)\n",
    "                # [32, 32]\n",
    "                pos_labels = torch.ones(pos_logits.shape[0]).long()\n",
    "                neg_labels = torch.zeros(neg_logits.shape[0]).long() \n",
    "                # TODO: stack pos and neg logits and labels (double check dim)\n",
    "                logits = torch.stack([pos_logits, neg_logits]).squeeze(0)\n",
    "                labels = torch.stack([pos_labels, neg_labels]).squeeze(0)\n",
    "                '''\n",
    "                \n",
    "                # pass into the loss function\n",
    "                encoding_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "                ''' Combine encoding loss with rl loss below '''\n",
    "                losses.append(encoding_loss)\n",
    "\n",
    "                # Then pass that encoding through the conv_net to get Q value estimates\n",
    "                Qs = self.conv_net(z_a)\n",
    "                next_Qs = self.target(z_next).detach().max(1)[0]\n",
    "            \n",
    "            else:\n",
    "                ''' If not using the encoder, pass the obs directly to the CNN '''\n",
    "                obs = process_obs(obs)\n",
    "                # estimate current q values from observations\n",
    "                Qs = self.conv_net(obs)\n",
    "                # find next max q values based on next observations\n",
    "                next_Qs = self.target(next_obs).detach().max(1)[0]\n",
    "            \n",
    "            ''' find target q values ''' \n",
    "            next_Qs = next_Qs.cpu().numpy() \n",
    "            Qs = torch.gather(Qs, dim=1, index=torch.tensor(action_list, dtype=torch.int64).to(device)).to(device)\n",
    "            target_Qs = torch.tensor(reward_list.squeeze(-1) + GAMMA * next_Qs).long().to(device)\n",
    "            ''' try to set Qs equal to target_Qs '''\n",
    "            q_loss = self.loss_fn(Qs, target_Qs).long()\n",
    "            losses.append(q_loss)\n",
    "            \n",
    "            ''' Loss update for q network and encoder head '''\n",
    "            losses = torch.stack(losses).sum()\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.num_param_updates += 1\n",
    "            if self.num_param_updates % target_update_freq == 0:\n",
    "                self.target.load_state_dict(self.conv_net.state_dict())\n",
    "\n",
    "    # TODO: check the sizes of inputs and outputs\n",
    "    def get_action(self, obs, epsilon, use_encoding=True):\n",
    "        ''' \n",
    "         If using an encoder, need to pass that thorugh the encoder\n",
    "         then use the encoding to pass through self.conv_net\n",
    "        '''\n",
    "        # obs = torch.tensor(obs)\n",
    "        if use_encoding:\n",
    "            obs = process_obs(obs)\n",
    "            obs = self.CURL.encode(obs).detach()\n",
    "\n",
    "        # Dividing obs by 255 is handled in encoder forward (only needed for use_encoding=False)\n",
    "        if len(obs.shape) == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        # epsilon greedy for selecting which action to take\n",
    "        if random.random() > epsilon:\n",
    "            qs = self.conv_net(obs)\n",
    "            action = qs[0].argmax().data.item()\n",
    "        else:\n",
    "            action = random.randint(0, self.N_action-1)\n",
    "\n",
    "        return action\n",
    "\n",
    "def get_decay(epi_iter):\n",
    "    decay = math.pow(0.999, epi_iter)\n",
    "    if decay < 0.05:\n",
    "        decay = 0.05\n",
    "    return decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1WWwX8wewOmy"
   },
   "outputs": [],
   "source": [
    "def main(aug=random_shift, train_curve_filename=\"default_curl_4000MC_400E\"):\n",
    "    env = gym.make('ALE/SpaceInvaders-v5')\n",
    "    max_epi_iter = 4000\n",
    "    max_MC_iter = 400\n",
    "    obs = env.render(mode='rgb_array')\n",
    "    encoder = PixelEncoder(obs_shape=obs.shape, feature_dim=50, num_layers=2, num_filters=64, output_logits=False).to(device)\n",
    "    CURL_encoder = CURL(obs_shape=obs.shape, z_dim=50, batch_size=1, encoder=encoder, output_type=\"continuous\").to(device)\n",
    "    agent = Agent(act_dim=env.action_space.n, max_epi_num=10000, max_epi_len=max_MC_iter, CURL=CURL_encoder, aug=aug)\n",
    "    train_curve = []\n",
    "    for epi_iter in range(max_epi_iter):\n",
    "        random.seed()\n",
    "        env.reset()\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        returns = 0.0\n",
    "        for MC_iter in range(max_MC_iter):\n",
    "            action = agent.get_action(obs, get_decay(epi_iter))\n",
    "            next_obs, reward, done = take_action(env, action)\n",
    "            returns += reward * agent.gamma ** (MC_iter)\n",
    "            agent.remember(obs, action, reward, next_obs)\n",
    "            obs = next_obs.copy()\n",
    "            if done or MC_iter >= max_MC_iter-1:\n",
    "                agent.buffer.create_new_epi()\n",
    "                break\n",
    "        print('Episode', epi_iter, 'returns', returns)\n",
    "        if epi_iter % 1 == 0:\n",
    "            train_curve.append(returns)\n",
    "        if epi_iter % 100 == 0:\n",
    "            np.save(train_curve_filename, np.array(train_curve))\n",
    "        if agent.buffer.is_available():\n",
    "            for _ in range(1):\n",
    "                agent.train()\n",
    "    print(train_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PpyPzumawOmz",
    "outputId": "e436d671-439f-408c-d4f2-ecac8fd5e2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 returns 47.62933558298766\n",
      "Episode 1 returns 19.986401756224492\n",
      "Episode 2 returns 12.773375104242554\n",
      "Episode 3 returns 11.0215421422098\n",
      "Episode 4 returns 37.43733713797183\n",
      "Episode 5 returns 20.56708221740586\n",
      "Episode 6 returns 21.13421759912679\n",
      "Episode 7 returns 18.89312287106762\n",
      "Episode 8 returns 24.08168057818884\n",
      "Episode 9 returns 31.207427442596916\n",
      "Episode 10 returns 31.829641905082656\n",
      "Episode 11 returns 13.718983061147078\n",
      "Episode 12 returns 28.556222509527128\n",
      "Episode 13 returns 23.240292634052818\n",
      "Episode 14 returns 29.293520541116557\n",
      "Episode 15 returns 29.63166835913544\n",
      "Episode 16 returns 30.20027038243237\n",
      "Episode 17 returns 51.26266977187717\n",
      "Episode 18 returns 36.56622301374786\n",
      "Episode 19 returns 44.84584305948491\n",
      "Episode 20 returns 36.749405038238585\n",
      "Episode 21 returns 17.026226217450255\n",
      "Episode 22 returns 13.17765949774962\n",
      "Episode 23 returns 36.264631406580904\n",
      "Episode 24 returns 25.422499725540515\n",
      "Episode 25 returns 10.538857629878834\n",
      "Episode 26 returns 27.192200213427984\n",
      "Episode 27 returns 17.3239255224248\n",
      "Episode 28 returns 17.076480644016787\n",
      "Episode 29 returns 4.780678753933239\n",
      "Episode 30 returns 28.814311766257752\n",
      "Episode 31 returns 15.030994099905685\n",
      "Episode 32 returns 17.315818999889476\n",
      "Episode 33 returns 26.071488766697357\n",
      "Episode 34 returns 28.86930764053242\n",
      "Episode 35 returns 28.643519372401485\n",
      "Episode 36 returns 5.338061744694143\n",
      "Episode 37 returns 23.35954227339829\n",
      "Episode 38 returns 38.043934711615\n",
      "Episode 39 returns 15.519156259055679\n",
      "Episode 40 returns 12.234276154716444\n",
      "Episode 41 returns 30.300870894454928\n",
      "Episode 42 returns 51.289746867292735\n",
      "Episode 43 returns 10.582490541069173\n",
      "Episode 44 returns 10.200056111645178\n",
      "Episode 45 returns 14.901514330061143\n",
      "Episode 46 returns 47.07833767628362\n",
      "Episode 47 returns 9.84305028357612\n",
      "Episode 48 returns 23.770637867967288\n",
      "Episode 49 returns 21.20187881089834\n",
      "Episode 50 returns 12.084132712436343\n",
      "Episode 51 returns 22.963408114028404\n",
      "Episode 52 returns 13.898650017087501\n",
      "Episode 53 returns 10.124412897912023\n",
      "Episode 54 returns 28.482953305744655\n",
      "Episode 55 returns 36.78211292635832\n",
      "Episode 56 returns 15.205595661906491\n",
      "Episode 57 returns 28.154732072533072\n",
      "Episode 58 returns 11.150204129605807\n",
      "Episode 59 returns 20.32418896446402\n",
      "Episode 60 returns 10.032504049721604\n",
      "Episode 61 returns 11.413178279137767\n",
      "Episode 62 returns 30.259008013623987\n",
      "Episode 63 returns 30.72703737250432\n",
      "Episode 64 returns 18.003559423502097\n",
      "Episode 65 returns 17.67565928616624\n",
      "Episode 66 returns 39.01107905880221\n",
      "Episode 67 returns 6.827854907015358\n",
      "Episode 68 returns 31.562000046919106\n",
      "Episode 69 returns 2.4970938279266344\n",
      "Episode 70 returns 9.645428763847613\n",
      "Episode 71 returns 19.859259536984705\n",
      "Episode 72 returns 22.877535406523286\n",
      "Episode 73 returns 29.809850149926206\n",
      "Episode 74 returns 20.197519106271184\n",
      "Episode 75 returns 24.189844755665355\n",
      "Episode 76 returns 15.42245694916558\n",
      "Episode 77 returns 7.8836725866935975\n",
      "Episode 78 returns 48.330450637319785\n",
      "Episode 79 returns 17.918780461066287\n",
      "Episode 80 returns 25.23049410600475\n",
      "Episode 81 returns 42.181590331873714\n",
      "Episode 82 returns 65.25279963025014\n",
      "Episode 83 returns 13.243810891969405\n",
      "Episode 84 returns 28.874305704678704\n",
      "Episode 85 returns 18.981413452586864\n",
      "Episode 86 returns 38.92475084754383\n",
      "Episode 87 returns 10.4827273115991\n",
      "Episode 88 returns 33.769079308830825\n",
      "Episode 89 returns 17.36441181822801\n",
      "Episode 90 returns 19.97149279198826\n",
      "Episode 91 returns 44.04863948549583\n",
      "Episode 92 returns 12.39716629062794\n",
      "Episode 93 returns 25.76180954303445\n",
      "Episode 94 returns 34.0484662128934\n",
      "Episode 95 returns 8.275425160672619\n",
      "Episode 96 returns 17.78712274357577\n",
      "Episode 97 returns 29.53087246109469\n",
      "Episode 98 returns 28.822976098608255\n",
      "Episode 99 returns 5.09573877097459\n",
      "Episode 100 returns 10.572602060976289\n",
      "Episode 101 returns 23.223699497894604\n",
      "Episode 102 returns 28.637632957626618\n",
      "Episode 103 returns 8.885109919297259\n",
      "Episode 104 returns 10.61863594674673\n",
      "Episode 105 returns 25.172982059493688\n",
      "Episode 106 returns 14.906754018908387\n",
      "Episode 107 returns 11.724743660080549\n",
      "Episode 108 returns 20.638141567308367\n",
      "Episode 109 returns 44.785763078354734\n",
      "Episode 110 returns 18.05259768790664\n",
      "Episode 111 returns 26.644718870098256\n",
      "Episode 112 returns 40.777883770664026\n",
      "Episode 113 returns 27.138316169664748\n",
      "Episode 114 returns 22.17165133274024\n",
      "Episode 115 returns 28.77384159284254\n",
      "Episode 116 returns 25.674630104693005\n",
      "Episode 117 returns 32.94605730894788\n",
      "Episode 118 returns 8.356074154915165\n",
      "Episode 119 returns 18.12189853504055\n",
      "Episode 120 returns 17.8388074291737\n",
      "Episode 121 returns 30.48112552730901\n",
      "Episode 122 returns 36.59610892916707\n",
      "Episode 123 returns 15.934534531219146\n",
      "Episode 124 returns 20.891727874228437\n",
      "Episode 125 returns 33.49252518798588\n",
      "Episode 126 returns 38.69493595377212\n",
      "Episode 127 returns 35.16488587859201\n",
      "Episode 128 returns 21.198094717111637\n",
      "Episode 129 returns 18.601651952008233\n",
      "Episode 130 returns 15.841209127284941\n",
      "Episode 131 returns 32.65353416213175\n",
      "Episode 132 returns 24.839284862514894\n",
      "Episode 133 returns 10.749221460310332\n",
      "Episode 134 returns 39.093091354697066\n",
      "Episode 135 returns 13.97032261896688\n",
      "Episode 136 returns 36.24129919080479\n",
      "Episode 137 returns 25.975183790460427\n",
      "Episode 138 returns 20.7585815957771\n",
      "Episode 139 returns 13.212916495722967\n",
      "Episode 140 returns 22.537789010304056\n",
      "Episode 141 returns 22.99971426034088\n",
      "Episode 142 returns 25.328788854045452\n",
      "Episode 143 returns 26.198862426303375\n",
      "Episode 144 returns 38.00055837366332\n",
      "Episode 145 returns 25.68118836076943\n",
      "Episode 146 returns 10.602090408122992\n",
      "Episode 147 returns 22.65116551418349\n",
      "Episode 148 returns 18.425216243302046\n",
      "Episode 149 returns 33.038892863234665\n",
      "Episode 150 returns 21.18076171546243\n",
      "Episode 151 returns 40.95392756570076\n",
      "Episode 152 returns 7.943852771280032\n",
      "Episode 153 returns 35.150858219010956\n",
      "Episode 154 returns 26.11397800539313\n",
      "Episode 155 returns 7.622520457818872\n",
      "Episode 156 returns 16.22871162900385\n",
      "Episode 157 returns 5.340183085915513\n",
      "Episode 158 returns 31.223837829108493\n",
      "Episode 159 returns 63.6629658916274\n",
      "Episode 160 returns 13.643994526934444\n",
      "Episode 161 returns 19.25104615821749\n",
      "Episode 162 returns 30.15426773687412\n",
      "Episode 163 returns 17.759649573618297\n",
      "Episode 164 returns 23.650186456123787\n",
      "Episode 165 returns 9.15579483842427\n",
      "Episode 166 returns 12.168682937824295\n",
      "Episode 167 returns 31.19990088527594\n",
      "Episode 168 returns 18.655283987311083\n",
      "Episode 169 returns 38.91343236788623\n",
      "Episode 170 returns 11.819791673246812\n",
      "Episode 171 returns 28.41037277017065\n",
      "Episode 172 returns 32.04635648787491\n",
      "Episode 173 returns 39.23817123388696\n",
      "Episode 174 returns 10.916582054761596\n",
      "Episode 175 returns 15.751769914423555\n",
      "Episode 176 returns 11.892227250804071\n",
      "Episode 177 returns 23.995011533692256\n",
      "Episode 178 returns 17.36986609224135\n",
      "Episode 179 returns 13.20419908314191\n",
      "Episode 180 returns 31.130680809590302\n",
      "Episode 181 returns 13.813480880326056\n",
      "Episode 182 returns 39.55249842430498\n",
      "Episode 183 returns 46.08176951594281\n",
      "Episode 184 returns 26.692358943304693\n",
      "Episode 185 returns 9.850571824323314\n",
      "Episode 186 returns 12.542710651833325\n",
      "Episode 187 returns 21.44305465668989\n",
      "Episode 188 returns 10.863256548359066\n",
      "Episode 189 returns 22.759827768567543\n",
      "Episode 190 returns 17.717491212605704\n",
      "Episode 191 returns 35.21161665288531\n",
      "Episode 192 returns 30.57012569577984\n",
      "Episode 193 returns 1.212083230222901\n",
      "Episode 194 returns 10.121463583098603\n",
      "Episode 195 returns 13.552959471458605\n",
      "Episode 196 returns 25.519376161441613\n",
      "Episode 197 returns 15.061238140818642\n",
      "Episode 198 returns 23.13754993911802\n",
      "Episode 199 returns 5.710695860380724\n",
      "Episode 200 returns 32.08018633140776\n",
      "Episode 201 returns 11.250498683137003\n",
      "Episode 202 returns 16.17797262419205\n",
      "Episode 203 returns 11.918321459309203\n",
      "Episode 204 returns 24.661029901841317\n",
      "Episode 205 returns 58.29177418324747\n",
      "Episode 206 returns 7.541406013728605\n",
      "Episode 207 returns 27.675249110295656\n",
      "Episode 208 returns 11.460143899385606\n",
      "Episode 209 returns 13.145232809262346\n",
      "Episode 210 returns 27.511534615523054\n",
      "Episode 211 returns 27.937943564768414\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4364/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4364/2819243854.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(aug, train_curve_filename)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mMC_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_MC_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepi_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mreturns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMC_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4364/1801724452.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs230proj/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs230proj/lib/python3.8/site-packages/gym/envs/atari/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Render rgb array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(color_jitter, \"color_jitter_4000MC_400E\")\n",
    "main(random_elastic_transform, \"random_elastic_transform_4000MC_400E\")\n",
    "main(random_fisheye, \"random_fisheye_4000MC_400E\")\n",
    "main(random_color_equalize, \"random_color_equalize_4000MC_400E\")\n",
    "main(random_gaussian_blur, \"random_gaussian_blur_4000MC_400E\")\n",
    "main(random_gaussian_noise, \"random_gaussian_noise_4000MC_400E\")\n",
    "main(random_horizontal_flip, \"random_horizontal_flip_4000MC_400E\")\n",
    "main(random_color_invert, \"random_color_invert_4000MC_400E\")\n",
    "main(random_perspective_shift, \"random_perspective_shift_4000MC_400E\")\n",
    "main(random_shift, \"random_shift_4000MC_400E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTeJzO4RwOmy"
   },
   "outputs": [],
   "source": [
    "def old_main():\n",
    "    env = gym.make('MountainCars-v0')\n",
    "    if len(env.observation_space.shape) >= 3:\n",
    "        env = WrapAtariEnv(env=env, noop_max=30, frameskip=3, framestack=4, test=test)\n",
    "    if not test:\n",
    "        dele = input(\"Do you wanna recreate ckpt and log folders? (y/n)\")\n",
    "        if dele == 'y':\n",
    "            if os.path.exists(save_dir):\n",
    "                shutil.rmtree(save_dir)\n",
    "\n",
    "    env = wrap_env(env, train=not test)\n",
    "    print(env.observation_space.shape)\n",
    "    if len(env.observation_space.shape) >= 3:\n",
    "        q_net = DQNetworkConv\n",
    "    else:\n",
    "        assert(False)\n",
    "    agent = DQNAgent(env=env, qnet=q_net)\n",
    "    if args.test:\n",
    "        agent.rollout(episodes=100, render=render)\n",
    "    else:\n",
    "        agent.train()\n",
    "    agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXyTh8bdwOmz"
   },
   "outputs": [],
   "source": [
    "def main_mountaincar():\n",
    "    env = gym.make('ALE/SpaceInvaders-v5')\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(5):\n",
    "        env.step(env.action_space.sample())\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        print(\"Step \", i, obs)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fC5kGapwOmz"
   },
   "outputs": [],
   "source": [
    "main_mountaincar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkCfk2GjBNRl"
   },
   "source": [
    "Code references for DQN:\n",
    "\n",
    "https://github.com/taochenshh/dqn-pytorch\n",
    "\n",
    "https://github.com/transedward/pytorch-dqn (for sampling from replay buffer)\n",
    "\n",
    "CURL code: https://github.com/MishaLaskin/curl"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sample_gym_env_v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cs230proj] *",
   "language": "python",
   "name": "conda-env-cs230proj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
